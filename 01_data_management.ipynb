{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Step 1: データ管理の基礎\n",
    "\n",
    "機械学習プロジェクトにおいて、**データ管理**は最も重要な基礎スキルの一つです。\n",
    "\n",
    "## 🎯 このノートブックで学ぶこと\n",
    "- ローカルデータの作成と確認\n",
    "- S3への効率的なデータアップロード\n",
    "- S3からのデータダウンロード\n",
    "- データのバージョン管理\n",
    "- 実際のプロジェクトでのベストプラクティス\n",
    "\n",
    "## ⏱️ 実行時間: 約5分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定とライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# SageMaker設定\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(\"🔧 データ管理環境の設定\")\n",
    "print(f\"📍 Region: {region}\")\n",
    "print(f\"🪣 S3 Bucket: {bucket}\")\n",
    "print(f\"👤 Role: {role.split('/')[-1]}\")\n",
    "print(f\"📅 現在時刻: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ローカルデータの作成と確認\n",
    "\n",
    "### 📚 学習ポイント\n",
    "- データの生成と保存\n",
    "- ファイルサイズとデータ形式の確認\n",
    "- データ品質のチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データが存在しない場合は生成\n",
    "if not os.path.exists('./data/train_lecture.csv'):\n",
    "    print(\"📊 講義用データセットを生成中...\")\n",
    "    import subprocess\n",
    "    result = subprocess.run(['python3', './data/create_lecture_dataset.py'], \n",
    "                          cwd='./data', capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"✅ データセットは既に存在します\")\n",
    "\n",
    "# ローカルデータの詳細確認\n",
    "print(\"\\n📋 ローカルデータの詳細情報:\")\n",
    "data_files = ['train_lecture.csv', 'validation_lecture.csv', 'test_lecture.csv']\n",
    "\n",
    "total_size = 0\n",
    "for file_name in data_files:\n",
    "    file_path = f'./data/{file_name}'\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        total_size += file_size\n",
    "        \n",
    "        # データを読み込んで基本情報を表示\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"\\n📄 {file_name}:\")\n",
    "        print(f\"  📏 ファイルサイズ: {file_size/1024:.1f} KB\")\n",
    "        print(f\"  📊 データ形状: {df.shape}\")\n",
    "        print(f\"  🔍 欠損値: {df.isnull().sum().sum()}個\")\n",
    "        print(f\"  📈 クラス分布: {dict(df['target'].value_counts().sort_index())}\")\n",
    "        \n",
    "        # メモリ使用量\n",
    "        memory_usage = df.memory_usage(deep=True).sum()\n",
    "        print(f\"  💾 メモリ使用量: {memory_usage/1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\n📦 総データサイズ: {total_size/1024:.1f} KB\")\n",
    "print(f\"💡 データ管理のポイント: 小さなデータでも構造化して管理することが重要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S3へのデータアップロード\n",
    "\n",
    "### 📚 学習ポイント\n",
    "- S3の階層構造の設計\n",
    "- データのバージョン管理\n",
    "- アップロード時間の測定\n",
    "- メタデータの付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3のデータ構造を設計\n",
    "project_name = 'ml-lecture'\n",
    "version = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "s3_prefix = f'{project_name}/data/v{version}'\n",
    "\n",
    "print(f\"🗂️ S3データ構造の設計:\")\n",
    "print(f\"  📁 プロジェクト: {project_name}\")\n",
    "print(f\"  🏷️ バージョン: v{version}\")\n",
    "print(f\"  📍 S3パス: s3://{bucket}/{s3_prefix}/\")\n",
    "\n",
    "# データアップロードの実行\n",
    "print(f\"\\n📤 S3へのデータアップロード開始...\")\n",
    "upload_start_time = time.time()\n",
    "\n",
    "s3_paths = {}\n",
    "upload_info = []\n",
    "\n",
    "for file_name in data_files:\n",
    "    local_path = f'./data/{file_name}'\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"  📤 {file_name} をアップロード中...\")\n",
    "        \n",
    "        # ファイル別のアップロード時間測定\n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        # データタイプに応じたS3パスを設定\n",
    "        data_type = file_name.replace('_lecture.csv', '')\n",
    "        s3_path = sagemaker_session.upload_data(\n",
    "            path=local_path,\n",
    "            bucket=bucket,\n",
    "            key_prefix=f'{s3_prefix}/{data_type}'\n",
    "        )\n",
    "        \n",
    "        file_upload_time = time.time() - file_start_time\n",
    "        file_size = os.path.getsize(local_path)\n",
    "        \n",
    "        s3_paths[data_type] = s3_path\n",
    "        upload_info.append({\n",
    "            'file': file_name,\n",
    "            'size_kb': file_size/1024,\n",
    "            'upload_time': file_upload_time,\n",
    "            's3_path': s3_path\n",
    "        })\n",
    "        \n",
    "        print(f\"    ✅ 完了 ({file_upload_time:.2f}秒, {file_size/1024:.1f}KB)\")\n",
    "\n",
    "total_upload_time = time.time() - upload_start_time\n",
    "\n",
    "print(f\"\\n📊 アップロード結果サマリー:\")\n",
    "print(f\"  ⏱️ 総アップロード時間: {total_upload_time:.2f}秒\")\n",
    "print(f\"  📦 総ファイル数: {len(upload_info)}個\")\n",
    "print(f\"  📏 総データサイズ: {sum([info['size_kb'] for info in upload_info]):.1f}KB\")\n",
    "print(f\"  🚀 平均アップロード速度: {sum([info['size_kb'] for info in upload_info])/total_upload_time:.1f}KB/秒\")\n",
    "\n",
    "# S3パスの保存（後で使用）\n",
    "print(f\"\\n🔗 S3データパス:\")\n",
    "for data_type, path in s3_paths.items():\n",
    "    print(f\"  {data_type}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S3からのデータダウンロード\n",
    "\n",
    "### 📚 学習ポイント\n",
    "- S3からの効率的なデータ取得\n",
    "- ダウンロード vs 直接読み込みの使い分け\n",
    "- データ整合性の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3からのデータダウンロードテスト\n",
    "print(\"📥 S3からのデータダウンロードテスト:\")\n",
    "\n",
    "# 一時的なダウンロードディレクトリを作成\n",
    "download_dir = './temp_download'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "download_start_time = time.time()\n",
    "\n",
    "# 方法1: sagemaker.Session.download_data()を使用\n",
    "print(\"\\n📥 方法1: SageMaker Session経由でダウンロード\")\n",
    "for data_type, s3_path in s3_paths.items():\n",
    "    print(f\"  📥 {data_type} をダウンロード中...\")\n",
    "    \n",
    "    file_start_time = time.time()\n",
    "    \n",
    "    # S3パスからバケット名とキーを抽出\n",
    "    s3_parts = s3_path.replace('s3://', '').split('/')\n",
    "    bucket_name = s3_parts[0]\n",
    "    s3_key = '/'.join(s3_parts[1:])\n",
    "    \n",
    "    # ダウンロード実行\n",
    "    local_download_path = sagemaker_session.download_data(\n",
    "        path=download_dir,\n",
    "        bucket=bucket_name,\n",
    "        key_prefix=s3_key.rsplit('/', 1)[0]  # ファイル名を除いたパス\n",
    "    )\n",
    "    \n",
    "    download_time = time.time() - file_start_time\n",
    "    print(f\"    ✅ 完了 ({download_time:.2f}秒)\")\n",
    "    print(f\"    📁 ローカルパス: {local_download_path}\")\n",
    "\n",
    "# 方法2: pandas.read_csv()で直接読み込み\n",
    "print(f\"\\n📥 方法2: pandasで直接S3から読み込み\")\n",
    "direct_read_start = time.time()\n",
    "\n",
    "# 訓練データを直接読み込み\n",
    "train_s3_path = s3_paths['train']\n",
    "df_from_s3 = pd.read_csv(train_s3_path)\n",
    "\n",
    "direct_read_time = time.time() - direct_read_start\n",
    "print(f\"  ✅ 直接読み込み完了 ({direct_read_time:.2f}秒)\")\n",
    "print(f\"  📊 データ形状: {df_from_s3.shape}\")\n",
    "\n",
    "total_download_time = time.time() - download_start_time\n",
    "\n",
    "print(f\"\\n📊 ダウンロード結果比較:\")\n",
    "print(f\"  📥 ダウンロード方式: {total_download_time:.2f}秒\")\n",
    "print(f\"  📖 直接読み込み方式: {direct_read_time:.2f}秒\")\n",
    "print(f\"  💡 推奨: 小さなデータは直接読み込み、大きなデータはダウンロード\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. データ整合性の確認\n",
    "\n",
    "### 📚 学習ポイント\n",
    "- アップロード・ダウンロード後のデータ検証\n",
    "- ハッシュ値による整合性チェック\n",
    "- データ品質の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "print(\"🔍 データ整合性の確認:\")\n",
    "\n",
    "# 元のデータを読み込み\n",
    "original_df = pd.read_csv('./data/train_lecture.csv')\n",
    "\n",
    "# S3から読み込んだデータと比較\n",
    "print(f\"\\n📊 データ比較:\")\n",
    "print(f\"  📄 元データ形状: {original_df.shape}\")\n",
    "print(f\"  📄 S3データ形状: {df_from_s3.shape}\")\n",
    "print(f\"  ✅ 形状一致: {original_df.shape == df_from_s3.shape}\")\n",
    "\n",
    "# データ内容の比較\n",
    "data_equal = original_df.equals(df_from_s3)\n",
    "print(f\"  ✅ データ内容一致: {data_equal}\")\n",
    "\n",
    "if not data_equal:\n",
    "    # 差分の詳細確認\n",
    "    diff_mask = (original_df != df_from_s3).any(axis=1)\n",
    "    diff_count = diff_mask.sum()\n",
    "    print(f\"  ⚠️ 差分のある行数: {diff_count}\")\n",
    "\n",
    "# ハッシュ値による検証\n",
    "def calculate_dataframe_hash(df):\n",
    "    \"\"\"DataFrameのハッシュ値を計算\"\"\"\n",
    "    return hashlib.md5(pd.util.hash_pandas_object(df, index=True).values).hexdigest()\n",
    "\n",
    "original_hash = calculate_dataframe_hash(original_df)\n",
    "s3_hash = calculate_dataframe_hash(df_from_s3)\n",
    "\n",
    "print(f\"\\n🔐 ハッシュ値による検証:\")\n",
    "print(f\"  📄 元データハッシュ: {original_hash[:16]}...\")\n",
    "print(f\"  📄 S3データハッシュ: {s3_hash[:16]}...\")\n",
    "print(f\"  ✅ ハッシュ一致: {original_hash == s3_hash}\")\n",
    "\n",
    "# データ品質チェック\n",
    "print(f\"\\n🔍 データ品質チェック:\")\n",
    "quality_checks = {\n",
    "    '欠損値なし': df_from_s3.isnull().sum().sum() == 0,\n",
    "    '重複行なし': df_from_s3.duplicated().sum() == 0,\n",
    "    'ターゲット値正常': df_from_s3['target'].isin([0, 1, 2]).all(),\n",
    "    '数値列正常': df_from_s3.select_dtypes(include=[np.number]).notna().all().all()\n",
    "}\n",
    "\n",
    "for check_name, result in quality_checks.items():\n",
    "    status = \"✅\" if result else \"❌\"\n",
    "    print(f\"  {status} {check_name}: {result}\")\n",
    "\n",
    "print(f\"\\n🎉 データ管理の基礎完了！\")\n",
    "print(f\"💡 次のステップ: このS3データを使って機械学習を実行します\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. データ管理のベストプラクティス\n",
    "\n",
    "### 📚 実際のプロジェクトで重要なポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 データ管理のベストプラクティス:\")\n",
    "\n",
    "best_practices = {\n",
    "    \"🗂️ 構造化された命名規則\": {\n",
    "        \"説明\": \"プロジェクト/データタイプ/バージョンの階層構造\",\n",
    "        \"例\": f\"s3://{bucket}/{project_name}/data/v{version}/train/\",\n",
    "        \"メリット\": \"データの発見・管理が容易\"\n",
    "    },\n",
    "    \"🏷️ バージョン管理\": {\n",
    "        \"説明\": \"タイムスタンプまたはセマンティックバージョニング\",\n",
    "        \"例\": f\"v{version} または v1.0.0\",\n",
    "        \"メリット\": \"データの変更履歴を追跡可能\"\n",
    "    },\n",
    "    \"🔍 データ検証\": {\n",
    "        \"説明\": \"アップロード後の整合性チェック\",\n",
    "        \"例\": \"ハッシュ値比較、形状確認、品質チェック\",\n",
    "        \"メリット\": \"データ破損の早期発見\"\n",
    "    },\n",
    "    \"📊 メタデータ管理\": {\n",
    "        \"説明\": \"データの説明、作成日時、サイズ等の記録\",\n",
    "        \"例\": \"data_catalog.json, README.md\",\n",
    "        \"メリット\": \"データの理解と再利用が容易\"\n",
    "    },\n",
    "    \"🔐 アクセス制御\": {\n",
    "        \"説明\": \"適切な権限設定とセキュリティ\",\n",
    "        \"例\": \"IAMロール、バケットポリシー\",\n",
    "        \"メリット\": \"データの安全性確保\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for practice, details in best_practices.items():\n",
    "    print(f\"\\n{practice}\")\n",
    "    print(f\"  📝 説明: {details['説明']}\")\n",
    "    print(f\"  💡 例: {details['例']}\")\n",
    "    print(f\"  ✨ メリット: {details['メリット']}\")\n",
    "\n",
    "print(f\"\\n🎯 今回学んだデータ管理スキル:\")\n",
    "skills = [\n",
    "    \"ローカルデータの作成と品質確認\",\n",
    "    \"S3への効率的なアップロード\",\n",
    "    \"S3からの柔軟なデータ取得\",\n",
    "    \"データ整合性の検証\",\n",
    "    \"バージョン管理の実践\"\n",
    "]\n",
    "\n",
    "for i, skill in enumerate(skills, 1):\n",
    "    print(f\"  {i}. ✅ {skill}\")\n",
    "\n",
    "print(f\"\\n🚀 次のノートブック: 02_script_mode_training.ipynb\")\n",
    "print(f\"   今回作成したS3データを使ってScript Modeでの機械学習を実行します！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. クリーンアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一時ファイルのクリーンアップ\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(download_dir):\n",
    "    shutil.rmtree(download_dir)\n",
    "    print(f\"🗑️ 一時ダウンロードディレクトリを削除: {download_dir}\")\n",
    "\n",
    "print(f\"\\n✅ データ管理の基礎学習完了！\")\n",
    "print(f\"📊 作成されたS3データ:\")\n",
    "for data_type, path in s3_paths.items():\n",
    "    print(f\"  {data_type}: {path}\")\n",
    "    \n",
    "# 次のノートブック用にS3パスを保存\n",
    "s3_paths_file = 's3_data_paths.json'\n",
    "import json\n",
    "with open(s3_paths_file, 'w') as f:\n",
    "    json.dump(s3_paths, f, indent=2)\n",
    "    \n",
    "print(f\"\\n💾 S3パス情報を保存: {s3_paths_file}\")\n",
    "print(f\"🎯 次のステップ: 02_script_mode_training.ipynb を実行してください\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
