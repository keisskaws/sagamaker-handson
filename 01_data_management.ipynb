{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Step 1: ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®åŸºç¤\n",
    "\n",
    "æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãŠã„ã¦ã€**ãƒ‡ãƒ¼ã‚¿ç®¡ç†**ã¯æœ€ã‚‚é‡è¦ãªåŸºç¤ã‚¹ã‚­ãƒ«ã®ä¸€ã¤ã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã¶ã“ã¨\n",
    "- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã¨ç¢ºèª\n",
    "- S3ã¸ã®åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "- S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "- ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "- å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "## â±ï¸ å®Ÿè¡Œæ™‚é–“: ç´„5åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# SageMakerè¨­å®š\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(\"ğŸ”§ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ç’°å¢ƒã®è¨­å®š\")\n",
    "print(f\"ğŸ“ Region: {region}\")\n",
    "print(f\"ğŸª£ S3 Bucket: {bucket}\")\n",
    "print(f\"ğŸ‘¤ Role: {role.split('/')[-1]}\")\n",
    "print(f\"ğŸ“… ç¾åœ¨æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã¨ç¢ºèª\n",
    "\n",
    "### ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "- ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨ä¿å­˜\n",
    "- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ç¢ºèª\n",
    "- ãƒ‡ãƒ¼ã‚¿å“è³ªã®ãƒã‚§ãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç”Ÿæˆ\n",
    "if not os.path.exists('./data/train_lecture.csv'):\n",
    "    print(\"ğŸ“Š è¬›ç¾©ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆä¸­...\")\n",
    "    import subprocess\n",
    "    result = subprocess.run(['python3', './data/create_lecture_dataset.py'], \n",
    "                          cwd='./data', capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™\")\n",
    "\n",
    "# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ç¢ºèª\n",
    "print(\"\\nğŸ“‹ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°æƒ…å ±:\")\n",
    "data_files = ['train_lecture.csv', 'validation_lecture.csv', 'test_lecture.csv']\n",
    "\n",
    "total_size = 0\n",
    "for file_name in data_files:\n",
    "    file_path = f'./data/{file_name}'\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        total_size += file_size\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"\\nğŸ“„ {file_name}:\")\n",
    "        print(f\"  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size/1024:.1f} KB\")\n",
    "        print(f\"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}\")\n",
    "        print(f\"  ğŸ” æ¬ æå€¤: {df.isnull().sum().sum()}å€‹\")\n",
    "        print(f\"  ğŸ“ˆ ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(df['target'].value_counts().sort_index())}\")\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "        memory_usage = df.memory_usage(deep=True).sum()\n",
    "        print(f\"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage/1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_size/1024:.1f} KB\")\n",
    "print(f\"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®ãƒã‚¤ãƒ³ãƒˆ: å°ã•ãªãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ§‹é€ åŒ–ã—ã¦ç®¡ç†ã™ã‚‹ã“ã¨ãŒé‡è¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S3ã¸ã®ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "### ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "- S3ã®éšå±¤æ§‹é€ ã®è¨­è¨ˆ\n",
    "- ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n",
    "- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã®æ¸¬å®š\n",
    "- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä»˜ä¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’è¨­è¨ˆ\n",
    "project_name = 'ml-lecture'\n",
    "version = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "s3_prefix = f'{project_name}/data/v{version}'\n",
    "\n",
    "print(f\"ğŸ—‚ï¸ S3ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è¨­è¨ˆ:\")\n",
    "print(f\"  ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_name}\")\n",
    "print(f\"  ğŸ·ï¸ ãƒãƒ¼ã‚¸ãƒ§ãƒ³: v{version}\")\n",
    "print(f\"  ğŸ“ S3ãƒ‘ã‚¹: s3://{bucket}/{s3_prefix}/\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ\n",
    "print(f\"\\nğŸ“¤ S3ã¸ã®ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹...\")\n",
    "upload_start_time = time.time()\n",
    "\n",
    "s3_paths = {}\n",
    "upload_info = []\n",
    "\n",
    "for file_name in data_files:\n",
    "    local_path = f'./data/{file_name}'\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"  ğŸ“¤ {file_name} ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚é–“æ¸¬å®š\n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸS3ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "        data_type = file_name.replace('_lecture.csv', '')\n",
    "        s3_path = sagemaker_session.upload_data(\n",
    "            path=local_path,\n",
    "            bucket=bucket,\n",
    "            key_prefix=f'{s3_prefix}/{data_type}'\n",
    "        )\n",
    "        \n",
    "        file_upload_time = time.time() - file_start_time\n",
    "        file_size = os.path.getsize(local_path)\n",
    "        \n",
    "        s3_paths[data_type] = s3_path\n",
    "        upload_info.append({\n",
    "            'file': file_name,\n",
    "            'size_kb': file_size/1024,\n",
    "            'upload_time': file_upload_time,\n",
    "            's3_path': s3_path\n",
    "        })\n",
    "        \n",
    "        print(f\"    âœ… å®Œäº† ({file_upload_time:.2f}ç§’, {file_size/1024:.1f}KB)\")\n",
    "\n",
    "total_upload_time = time.time() - upload_start_time\n",
    "\n",
    "print(f\"\\nğŸ“Š ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"  â±ï¸ ç·ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {total_upload_time:.2f}ç§’\")\n",
    "print(f\"  ğŸ“¦ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(upload_info)}å€‹\")\n",
    "print(f\"  ğŸ“ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {sum([info['size_kb'] for info in upload_info]):.1f}KB\")\n",
    "print(f\"  ğŸš€ å¹³å‡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é€Ÿåº¦: {sum([info['size_kb'] for info in upload_info])/total_upload_time:.1f}KB/ç§’\")\n",
    "\n",
    "# S3ãƒ‘ã‚¹ã®ä¿å­˜ï¼ˆå¾Œã§ä½¿ç”¨ï¼‰\n",
    "print(f\"\\nğŸ”— S3ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹:\")\n",
    "for data_type, path in s3_paths.items():\n",
    "    print(f\"  {data_type}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "### ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "- S3ã‹ã‚‰ã®åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ vs ç›´æ¥èª­ã¿è¾¼ã¿ã®ä½¿ã„åˆ†ã‘\n",
    "- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ğŸ“¥ S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ:\")\n",
    "\n",
    "# ä¸€æ™‚çš„ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "download_dir = './temp_download'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "download_start_time = time.time()\n",
    "\n",
    "# æ–¹æ³•1: sagemaker.Session.download_data()ã‚’ä½¿ç”¨\n",
    "print(\"\\nğŸ“¥ æ–¹æ³•1: SageMaker SessionçµŒç”±ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\n",
    "for data_type, s3_path in s3_paths.items():\n",
    "    print(f\"  ğŸ“¥ {data_type} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    \n",
    "    file_start_time = time.time()\n",
    "    \n",
    "    # S3ãƒ‘ã‚¹ã‹ã‚‰ãƒã‚±ãƒƒãƒˆåã¨ã‚­ãƒ¼ã‚’æŠ½å‡º\n",
    "    s3_parts = s3_path.replace('s3://', '').split('/')\n",
    "    bucket_name = s3_parts[0]\n",
    "    s3_key = '/'.join(s3_parts[1:])\n",
    "    \n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ\n",
    "    local_download_path = sagemaker_session.download_data(\n",
    "        path=download_dir,\n",
    "        bucket=bucket_name,\n",
    "        key_prefix=s3_key.rsplit('/', 1)[0]  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤ã„ãŸãƒ‘ã‚¹\n",
    "    )\n",
    "    \n",
    "    download_time = time.time() - file_start_time\n",
    "    print(f\"    âœ… å®Œäº† ({download_time:.2f}ç§’)\")\n",
    "    print(f\"    ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹: {local_download_path}\")\n",
    "\n",
    "# æ–¹æ³•2: pandas.read_csv()ã§ç›´æ¥èª­ã¿è¾¼ã¿\n",
    "print(f\"\\nğŸ“¥ æ–¹æ³•2: pandasã§ç›´æ¥S3ã‹ã‚‰èª­ã¿è¾¼ã¿\")\n",
    "direct_read_start = time.time()\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥èª­ã¿è¾¼ã¿\n",
    "train_s3_path = s3_paths['train']\n",
    "df_from_s3 = pd.read_csv(train_s3_path)\n",
    "\n",
    "direct_read_time = time.time() - direct_read_start\n",
    "print(f\"  âœ… ç›´æ¥èª­ã¿è¾¼ã¿å®Œäº† ({direct_read_time:.2f}ç§’)\")\n",
    "print(f\"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_from_s3.shape}\")\n",
    "\n",
    "total_download_time = time.time() - download_start_time\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœæ¯”è¼ƒ:\")\n",
    "print(f\"  ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹å¼: {total_download_time:.2f}ç§’\")\n",
    "print(f\"  ğŸ“– ç›´æ¥èª­ã¿è¾¼ã¿æ–¹å¼: {direct_read_time:.2f}ç§’\")\n",
    "print(f\"  ğŸ’¡ æ¨å¥¨: å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯ç›´æ¥èª­ã¿è¾¼ã¿ã€å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºèª\n",
    "\n",
    "### ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã®ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼\n",
    "- ãƒãƒƒã‚·ãƒ¥å€¤ã«ã‚ˆã‚‹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯\n",
    "- ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "print(\"ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºèª:\")\n",
    "\n",
    "# å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "original_df = pd.read_csv('./data/train_lecture.csv')\n",
    "\n",
    "# S3ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒ:\")\n",
    "print(f\"  ğŸ“„ å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {original_df.shape}\")\n",
    "print(f\"  ğŸ“„ S3ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_from_s3.shape}\")\n",
    "print(f\"  âœ… å½¢çŠ¶ä¸€è‡´: {original_df.shape == df_from_s3.shape}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å†…å®¹ã®æ¯”è¼ƒ\n",
    "data_equal = original_df.equals(df_from_s3)\n",
    "print(f\"  âœ… ãƒ‡ãƒ¼ã‚¿å†…å®¹ä¸€è‡´: {data_equal}\")\n",
    "\n",
    "if not data_equal:\n",
    "    # å·®åˆ†ã®è©³ç´°ç¢ºèª\n",
    "    diff_mask = (original_df != df_from_s3).any(axis=1)\n",
    "    diff_count = diff_mask.sum()\n",
    "    print(f\"  âš ï¸ å·®åˆ†ã®ã‚ã‚‹è¡Œæ•°: {diff_count}\")\n",
    "\n",
    "# ãƒãƒƒã‚·ãƒ¥å€¤ã«ã‚ˆã‚‹æ¤œè¨¼\n",
    "def calculate_dataframe_hash(df):\n",
    "    \"\"\"DataFrameã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—\"\"\"\n",
    "    return hashlib.md5(pd.util.hash_pandas_object(df, index=True).values).hexdigest()\n",
    "\n",
    "original_hash = calculate_dataframe_hash(original_df)\n",
    "s3_hash = calculate_dataframe_hash(df_from_s3)\n",
    "\n",
    "print(f\"\\nğŸ” ãƒãƒƒã‚·ãƒ¥å€¤ã«ã‚ˆã‚‹æ¤œè¨¼:\")\n",
    "print(f\"  ğŸ“„ å…ƒãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥: {original_hash[:16]}...\")\n",
    "print(f\"  ğŸ“„ S3ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥: {s3_hash[:16]}...\")\n",
    "print(f\"  âœ… ãƒãƒƒã‚·ãƒ¥ä¸€è‡´: {original_hash == s3_hash}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯\n",
    "print(f\"\\nğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯:\")\n",
    "quality_checks = {\n",
    "    'æ¬ æå€¤ãªã—': df_from_s3.isnull().sum().sum() == 0,\n",
    "    'é‡è¤‡è¡Œãªã—': df_from_s3.duplicated().sum() == 0,\n",
    "    'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤æ­£å¸¸': df_from_s3['target'].isin([0, 1, 2]).all(),\n",
    "    'æ•°å€¤åˆ—æ­£å¸¸': df_from_s3.select_dtypes(include=[np.number]).notna().all().all()\n",
    "}\n",
    "\n",
    "for check_name, result in quality_checks.items():\n",
    "    status = \"âœ…\" if result else \"âŒ\"\n",
    "    print(f\"  {status} {check_name}: {result}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®åŸºç¤å®Œäº†ï¼\")\n",
    "print(f\"ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã“ã®S3ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "### ğŸ“š å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:\")\n",
    "\n",
    "best_practices = {\n",
    "    \"ğŸ—‚ï¸ æ§‹é€ åŒ–ã•ã‚ŒãŸå‘½åè¦å‰‡\": {\n",
    "        \"èª¬æ˜\": \"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—/ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®éšå±¤æ§‹é€ \",\n",
    "        \"ä¾‹\": f\"s3://{bucket}/{project_name}/data/v{version}/train/\",\n",
    "        \"ãƒ¡ãƒªãƒƒãƒˆ\": \"ãƒ‡ãƒ¼ã‚¿ã®ç™ºè¦‹ãƒ»ç®¡ç†ãŒå®¹æ˜“\"\n",
    "    },\n",
    "    \"ğŸ·ï¸ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\": {\n",
    "        \"èª¬æ˜\": \"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¾ãŸã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°\",\n",
    "        \"ä¾‹\": f\"v{version} ã¾ãŸã¯ v1.0.0\",\n",
    "        \"ãƒ¡ãƒªãƒƒãƒˆ\": \"ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›´å±¥æ­´ã‚’è¿½è·¡å¯èƒ½\"\n",
    "    },\n",
    "    \"ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼\": {\n",
    "        \"èª¬æ˜\": \"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯\",\n",
    "        \"ä¾‹\": \"ãƒãƒƒã‚·ãƒ¥å€¤æ¯”è¼ƒã€å½¢çŠ¶ç¢ºèªã€å“è³ªãƒã‚§ãƒƒã‚¯\",\n",
    "        \"ãƒ¡ãƒªãƒƒãƒˆ\": \"ãƒ‡ãƒ¼ã‚¿ç ´æã®æ—©æœŸç™ºè¦‹\"\n",
    "    },\n",
    "    \"ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†\": {\n",
    "        \"èª¬æ˜\": \"ãƒ‡ãƒ¼ã‚¿ã®èª¬æ˜ã€ä½œæˆæ—¥æ™‚ã€ã‚µã‚¤ã‚ºç­‰ã®è¨˜éŒ²\",\n",
    "        \"ä¾‹\": \"data_catalog.json, README.md\",\n",
    "        \"ãƒ¡ãƒªãƒƒãƒˆ\": \"ãƒ‡ãƒ¼ã‚¿ã®ç†è§£ã¨å†åˆ©ç”¨ãŒå®¹æ˜“\"\n",
    "    },\n",
    "    \"ğŸ” ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡\": {\n",
    "        \"èª¬æ˜\": \"é©åˆ‡ãªæ¨©é™è¨­å®šã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£\",\n",
    "        \"ä¾‹\": \"IAMãƒ­ãƒ¼ãƒ«ã€ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼\",\n",
    "        \"ãƒ¡ãƒªãƒƒãƒˆ\": \"ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨æ€§ç¢ºä¿\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for practice, details in best_practices.items():\n",
    "    print(f\"\\n{practice}\")\n",
    "    print(f\"  ğŸ“ èª¬æ˜: {details['èª¬æ˜']}\")\n",
    "    print(f\"  ğŸ’¡ ä¾‹: {details['ä¾‹']}\")\n",
    "    print(f\"  âœ¨ ãƒ¡ãƒªãƒƒãƒˆ: {details['ãƒ¡ãƒªãƒƒãƒˆ']}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ä»Šå›å­¦ã‚“ã ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¹ã‚­ãƒ«:\")\n",
    "skills = [\n",
    "    \"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã¨å“è³ªç¢ºèª\",\n",
    "    \"S3ã¸ã®åŠ¹ç‡çš„ãªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\",\n",
    "    \"S3ã‹ã‚‰ã®æŸ”è»Ÿãªãƒ‡ãƒ¼ã‚¿å–å¾—\",\n",
    "    \"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼\",\n",
    "    \"ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã®å®Ÿè·µ\"\n",
    "]\n",
    "\n",
    "for i, skill in enumerate(skills, 1):\n",
    "    print(f\"  {i}. âœ… {skill}\")\n",
    "\n",
    "print(f\"\\nğŸš€ æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: 02_script_mode_training.ipynb\")\n",
    "print(f\"   ä»Šå›ä½œæˆã—ãŸS3ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦Script Modeã§ã®æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(download_dir):\n",
    "    shutil.rmtree(download_dir)\n",
    "    print(f\"ğŸ—‘ï¸ ä¸€æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤: {download_dir}\")\n",
    "\n",
    "print(f\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®åŸºç¤å­¦ç¿’å®Œäº†ï¼\")\n",
    "print(f\"ğŸ“Š ä½œæˆã•ã‚ŒãŸS3ãƒ‡ãƒ¼ã‚¿:\")\n",
    "for data_type, path in s3_paths.items():\n",
    "    print(f\"  {data_type}: {path}\")\n",
    "    \n",
    "# æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”¨ã«S3ãƒ‘ã‚¹ã‚’ä¿å­˜\n",
    "s3_paths_file = 's3_data_paths.json'\n",
    "import json\n",
    "with open(s3_paths_file, 'w') as f:\n",
    "    json.dump(s3_paths, f, indent=2)\n",
    "    \n",
    "print(f\"\\nğŸ’¾ S3ãƒ‘ã‚¹æƒ…å ±ã‚’ä¿å­˜: {s3_paths_file}\")\n",
    "print(f\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: 02_script_mode_training.ipynb ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
