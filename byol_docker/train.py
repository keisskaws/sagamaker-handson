#!/usr/bin/env python3
"""
BYOL (Bring Your Own Library) ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸSageMakerãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
"""

import argparse
import os
import sys
import pandas as pd
import numpy as np
import joblib
import json
import time
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif

# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)
    print(f"ğŸ“ Pythonãƒ‘ã‚¹ã«è¿½åŠ : {current_dir}")

# ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
print("ğŸ” ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œä¸­...")
print(f"Pythonãƒ‘ã‚¹: {sys.path[:3]}...")  # æœ€åˆã®3ã¤ã‚’è¡¨ç¤º
print(f"ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir}")

# custom_ml_libãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèª
custom_lib_path = os.path.join(current_dir, 'custom_ml_lib')
print(f"custom_ml_libãƒ‘ã‚¹: {custom_lib_path}")
print(f"custom_ml_libå­˜åœ¨: {os.path.exists(custom_lib_path)}")

if os.path.exists(custom_lib_path):
    files = os.listdir(custom_lib_path)
    print(f"custom_ml_libå†…ãƒ•ã‚¡ã‚¤ãƒ«: {files}")

try:
    from custom_ml_lib.custom_classifier import CustomEnsembleClassifier
    from custom_ml_lib.custom_preprocessor import CustomPreprocessor
    from custom_ml_lib import utils as custom_utils
    print("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
    CUSTOM_LIB_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    print("æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™")
    CustomEnsembleClassifier = None
    CustomPreprocessor = None
    custom_utils = None
    CUSTOM_LIB_AVAILABLE = False

def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ"""
    parser = argparse.ArgumentParser()
    
    # SageMakerã®ç’°å¢ƒå¤‰æ•°
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--use-custom-ensemble', type=str, default='true')
    parser.add_argument('--use_custom_ensemble', type=str, default='true')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--ensemble-weights', type=str, default='0.4,0.4,0.2')
    parser.add_argument('--ensemble_weights', type=str, default='0.4,0.4,0.2')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--custom-preprocessing', type=str, default='true')
    parser.add_argument('--custom_preprocessing', type=str, default='true')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--ensemble-rf', type=str, default='true')
    parser.add_argument('--ensemble_rf', type=str, default='true')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--ensemble-gb', type=str, default='true')
    parser.add_argument('--ensemble_gb', type=str, default='true')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--ensemble-lr', type=str, default='true')
    parser.add_argument('--ensemble_lr', type=str, default='true')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--feature-selection-k', type=int, default=20)
    parser.add_argument('--feature_selection_k', type=int, default=20)  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    parser.add_argument('--n-jobs', type=int, default=-1)
    parser.add_argument('--n_jobs', type=int, default=-1)  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆ
    
    return parser.parse_args()

def load_data(train_path, test_path=None, validation_path=None):
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    datasets = {}
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    train_file = os.path.join(train_path, 'train_lecture.csv')
    if os.path.exists(train_file):
        datasets['train'] = pd.read_csv(train_file)
        print(f"  Train: {datasets['train'].shape}")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    if test_path:
        test_file = os.path.join(test_path, 'test_lecture.csv')
        if os.path.exists(test_file):
            datasets['test'] = pd.read_csv(test_file)
            print(f"  Test: {datasets['test'].shape}")
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    if validation_path:
        val_file = os.path.join(validation_path, 'validation_lecture.csv')
        if os.path.exists(val_file):
            datasets['validation'] = pd.read_csv(val_file)
            print(f"  Validation: {datasets['validation'].shape}")
    
    return datasets

def create_preprocessing_pipeline(feature_selection_k=20):
    """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ"""
    print(f"å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆä¸­... (ç‰¹å¾´é¸æŠ: {feature_selection_k})")
    
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('feature_selection', SelectKBest(f_classif, k=feature_selection_k))
    ])
    
    return pipeline

def train_custom_model(X_train, y_train, X_val=None, y_val=None, args=None):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
    print("\n=== BYOL ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")
    
    if CustomEnsembleClassifier is None:
        print("âŒ ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        return model, {"model_type": "fallback_random_forest"}
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡å™¨ã®è¨­å®š
    use_rf = args.ensemble_rf.lower() == 'true' if args else True
    use_gb = args.ensemble_gb.lower() == 'true' if args else True
    use_lr = args.ensemble_lr.lower() == 'true' if args else True
    
    print(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š:")
    print(f"  RandomForest: {use_rf}")
    print(f"  GradientBoosting: {use_gb}")
    print(f"  LogisticRegression: {use_lr}")
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = CustomEnsembleClassifier(
        use_rf=use_rf,
        use_gb=use_gb,
        use_lr=use_lr,
        rf_params={'n_estimators': 50, 'max_depth': 10},
        gb_params={'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.1},
        lr_params={'C': 1.0, 'penalty': 'l2'}
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    # è¨“ç·´ç²¾åº¦
    train_score = model.score(X_train, y_train)
    print(f"è¨“ç·´ç²¾åº¦: {train_score:.4f}")
    
    # æ¤œè¨¼ç²¾åº¦
    val_score = None
    if X_val is not None and y_val is not None:
        val_score = model.score(X_val, y_val)
        print(f"æ¤œè¨¼ç²¾åº¦: {val_score:.4f}")
    
    print(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“: {training_time:.2f}ç§’")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    try:
        importances = model.get_feature_importance()
        if importances:
            print("\nç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½5å€‹ï¼‰:")
            for model_name, importance in importances.items():
                if hasattr(importance, '__len__'):
                    top_indices = np.argsort(importance)[::-1][:5]
                    print(f"  {model_name}:")
                    for i, idx in enumerate(top_indices):
                        print(f"    {i+1}. ç‰¹å¾´é‡ {idx}: {importance[idx]:.4f}")
    except Exception as e:
        print(f"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—: {e}")
    
    results = {
        "model_type": "custom_ensemble",
        "training_time": training_time,
        "train_score": train_score,
        "val_score": val_score,
        "ensemble_config": {
            "use_rf": use_rf,
            "use_gb": use_gb,
            "use_lr": use_lr
        }
    }
    
    return model, results

def evaluate_model(model, X_test, y_test):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\nãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ä¸­...")
    
    predictions = model.predict(X_test)
    
    # äºˆæ¸¬ç¢ºç‡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    probabilities = None
    if hasattr(model, 'predict_proba'):
        try:
            probabilities = model.predict_proba(X_test)
        except Exception as e:
            print(f"äºˆæ¸¬ç¢ºç‡ã®å–å¾—ã«å¤±æ•—: {e}")
    
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {accuracy:.4f}")
    print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(y_test, predictions))
    
    print("\næ··åŒè¡Œåˆ—:")
    cm = confusion_matrix(y_test, predictions)
    print(cm)
    
    results = {
        'accuracy': accuracy,
        'predictions': predictions.tolist(),
    }
    
    if probabilities is not None:
        results['probabilities'] = probabilities.tolist()
    
    return results

def model_fn(model_dir):
    """SageMakeræ¨è«–ç”¨ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°"""
    model = joblib.load(os.path.join(model_dir, "model.joblib"))
    preprocessor = joblib.load(os.path.join(model_dir, "preprocessor.joblib"))
    return {'model': model, 'preprocessor': preprocessor}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” å¼•æ•°ã‚’è§£æä¸­...")
    args = parse_args()
    
    print(f"ğŸ“ å—ä¿¡ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    
    # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ç‰ˆã‚’å„ªå…ˆã—ã¦ä½¿ç”¨
    use_custom_ensemble = getattr(args, 'use_custom_ensemble', None) or getattr(args, 'use-custom-ensemble', 'true')
    ensemble_weights = getattr(args, 'ensemble_weights', None) or getattr(args, 'ensemble-weights', '0.4,0.4,0.2')
    custom_preprocessing = getattr(args, 'custom_preprocessing', None) or getattr(args, 'custom-preprocessing', 'true')
    n_jobs = getattr(args, 'n_jobs', None) or getattr(args, 'n-jobs', -1)
    feature_selection_k = getattr(args, 'feature_selection_k', None) or getattr(args, 'feature-selection-k', 20)
    
    print(f"  use-custom-ensemble: {use_custom_ensemble}")
    print(f"  ensemble-weights: {ensemble_weights}")
    print(f"  custom-preprocessing: {custom_preprocessing}")
    print(f"  n-jobs: {n_jobs}")
    print(f"  feature-selection-k: {feature_selection_k}")
    
    print("=== BYOL (Bring Your Own Library) ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")
    print("å¼•æ•°:")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    
    total_start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    datasets = load_data(args.train, args.test, args.validation)
    
    if 'train' not in datasets:
        print("âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
    preprocessor = create_preprocessing_pipeline(args.feature_selection_k)
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
    X_train = datasets['train'].drop('target', axis=1)
    y_train = datasets['train']['target']
    
    X_test = None
    y_test = None
    if 'test' in datasets:
        X_test = datasets['test'].drop('target', axis=1)
        y_test = datasets['test']['target']
    
    X_val = None
    y_val = None
    if 'validation' in datasets:
        X_val = datasets['validation'].drop('target', axis=1)
        y_val = datasets['validation']['target']
    
    print(f"\nãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"  è¨“ç·´: {X_train.shape}")
    if X_val is not None:
        print(f"  æ¤œè¨¼: {X_val.shape}")
    if X_test is not None:
        print(f"  ãƒ†ã‚¹ãƒˆ: {X_test.shape}")
    
    # å‰å‡¦ç†ã®å®Ÿè¡Œ
    print("\nå‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
    preprocessing_start = time.time()
    
    X_train_processed = preprocessor.fit_transform(X_train, y_train)
    if X_val is not None:
        X_val_processed = preprocessor.transform(X_val)
    else:
        X_val_processed = None
    if X_test is not None:
        X_test_processed = preprocessor.transform(X_test)
    else:
        X_test_processed = None
    
    preprocessing_time = time.time() - preprocessing_start
    print(f"å‰å‡¦ç†å®Œäº†: {preprocessing_time:.2f}ç§’")
    print(f"å‡¦ç†å¾Œã®ç‰¹å¾´é‡æ•°: {X_train_processed.shape[1]}")
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    if use_custom_ensemble.lower() == 'true':
        model, training_results = train_custom_model(
            X_train_processed, y_train, 
            X_val_processed, y_val,
            args
        )
    else:
        print("æ¨™æº–ã®RandomForestã‚’ä½¿ç”¨ã—ã¾ã™...")
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_processed, y_train)
        training_results = {"model_type": "standard_random_forest"}
    
    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    evaluation_results = {}
    if X_test_processed is not None and y_test is not None:
        evaluation_results = evaluate_model(model, X_test_processed, y_test)
    
    total_time = time.time() - total_start_time
    
    # çµæœã®ä¿å­˜
    print(f"\nãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
    joblib.dump(model, os.path.join(args.model_dir, "model.joblib"))
    joblib.dump(preprocessor, os.path.join(args.model_dir, "preprocessor.joblib"))
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜
    metrics = {
        'total_training_time': total_time,
        'preprocessing_time': preprocessing_time,
        'data_shape': {
            'train': list(X_train.shape),
            'features_after_preprocessing': X_train_processed.shape[1]
        },
        'training_results': training_results
    }
    
    if evaluation_results:
        metrics['evaluation_results'] = evaluation_results
    
    with open(os.path.join(args.model_dir, 'metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\n=== BYOL ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº† ===")
    print(f"ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {training_results.get('model_type', 'unknown')}")
    if evaluation_results:
        print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {evaluation_results.get('accuracy', 'N/A'):.4f}")

if __name__ == '__main__':
    try:
        print("ğŸš€ BYOL Training Script é–‹å§‹")
        print(f"Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
        print(f"ã‚«ã‚¹ã‚¿ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆ©ç”¨å¯èƒ½: {CUSTOM_LIB_AVAILABLE}")
        
        main()
        
        print("âœ… BYOL Training Script æ­£å¸¸çµ‚äº†")
        
    except Exception as e:
        print(f"âŒ BYOL Training Script ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
        
        import traceback
        print("ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
        traceback.print_exc()
        
        # ã‚¨ãƒ©ãƒ¼ã§çµ‚äº†
        sys.exit(1)
