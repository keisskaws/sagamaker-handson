{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ’» Step 2: Script Mode ã§ã®æ©Ÿæ¢°å­¦ç¿’\n",
    "\n",
    "å‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ä½œæˆãƒ»ç®¡ç†ã—ãŸS3ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€**Script Mode**ã§ã®æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã¶ã“ã¨\n",
    "- S3ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ´»ç”¨\n",
    "- ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§ã®æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "- è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã¨è©•ä¾¡\n",
    "- Script Modeã®ç‰¹å¾´ã¨åˆ©ç‚¹\n",
    "\n",
    "## â±ï¸ å®Ÿè¡Œæ™‚é–“: ç´„5-8åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨S3ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ’» Script Modeç’°å¢ƒã®è¨­å®š\")\n",
    "print(f\"ğŸ“… é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# å‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ä¿å­˜ã—ãŸS3ãƒ‘ã‚¹æƒ…å ±ã‚’èª­ã¿è¾¼ã¿\n",
    "try:\n",
    "    with open('s3_data_paths.json', 'r') as f:\n",
    "        s3_paths = json.load(f)\n",
    "    print(\"âœ… S3ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹æƒ…å ±ã‚’èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "    for data_type, path in s3_paths.items():\n",
    "        print(f\"  ğŸ“Š {data_type}: {path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ S3ãƒ‘ã‚¹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ”„ å…ˆã« 01_data_management.ipynb ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. S3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "\n",
    "### ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "- S3ã‹ã‚‰ç›´æ¥DataFrameã«èª­ã¿è¾¼ã¿\n",
    "- ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ç¢ºèª\n",
    "- èª­ã¿è¾¼ã¿æ™‚é–“ã®æ¸¬å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "data_load_start = time.time()\n",
    "\n",
    "# S3ã‹ã‚‰å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿\n",
    "datasets = {}\n",
    "for data_type, s3_path in s3_paths.items():\n",
    "    print(f\"  ğŸ“Š {data_type} ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "    \n",
    "    file_start = time.time()\n",
    "    datasets[data_type] = pd.read_csv(s3_path)\n",
    "    file_time = time.time() - file_start\n",
    "    \n",
    "    print(f\"    âœ… å®Œäº† ({file_time:.2f}ç§’, å½¢çŠ¶: {datasets[data_type].shape})\")\n",
    "\n",
    "data_load_time = time.time() - data_load_start\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:\")\n",
    "print(f\"  â±ï¸ ç·èª­ã¿è¾¼ã¿æ™‚é–“: {data_load_time:.2f}ç§’\")\n",
    "print(f\"  ğŸ“ˆ è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {datasets['train'].shape}\")\n",
    "print(f\"  ğŸ“‰ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {datasets['validation'].shape}\")\n",
    "print(f\"  ğŸ¯ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {datasets['test'].shape}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ç¢ºèª\n",
    "train_data = datasets['train']\n",
    "val_data = datasets['validation']\n",
    "test_data = datasets['test']\n",
    "\n",
    "print(f\"\\nğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª:\")\n",
    "print(f\"  ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆè¨“ç·´ï¼‰: {dict(train_data['target'].value_counts().sort_index())}\")\n",
    "print(f\"  ğŸ” æ¬ æå€¤ï¼ˆè¨“ç·´ï¼‰: {train_data.isnull().sum().sum()}å€‹\")\n",
    "print(f\"  ğŸ“ ç‰¹å¾´é‡æ•°: {len([col for col in train_data.columns if col != 'target'])}å€‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã¨ç†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ\n",
    "plt.subplot(1, 3, 1)\n",
    "train_data['target'].value_counts().plot(kind='bar', color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼‰')\n",
    "plt.xlabel('ã‚¯ãƒ©ã‚¹')\n",
    "plt.ylabel('ã‚µãƒ³ãƒ—ãƒ«æ•°')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# é‡è¦ç‰¹å¾´é‡ã®åˆ†å¸ƒ\n",
    "plt.subplot(1, 3, 2)\n",
    "for target in train_data['target'].unique():\n",
    "    subset = train_data[train_data['target'] == target]\n",
    "    plt.hist(subset['feature_00'].dropna(), alpha=0.7, label=f'Class {target}', bins=20)\n",
    "plt.title('é‡è¦ç‰¹å¾´é‡ã®åˆ†å¸ƒ')\n",
    "plt.xlabel('feature_00')\n",
    "plt.ylabel('é »åº¦')\n",
    "plt.legend()\n",
    "\n",
    "# æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "plt.subplot(1, 3, 3)\n",
    "missing_data = train_data.isnull().sum()\n",
    "missing_features = missing_data[missing_data > 0]\n",
    "if len(missing_features) > 0:\n",
    "    missing_features.plot(kind='bar')\n",
    "    plt.title('æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³')\n",
    "    plt.xlabel('ç‰¹å¾´é‡')\n",
    "    plt.ylabel('æ¬ æå€¤æ•°')\n",
    "    plt.xticks(rotation=45)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'æ¬ æå€¤ãªã—', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç†è§£å®Œäº†: S3ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’ç¢ºèªã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Script Mode ã§ã®æ©Ÿæ¢°å­¦ç¿’å®Ÿè¡Œ\n",
    "\n",
    "### ğŸ“š å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "- å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰\n",
    "- è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ\n",
    "- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "- çµæœã®è©•ä¾¡ã¨å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lecture_preprocessing_pipeline():\n",
    "    \"\"\"è¬›ç¾©ç”¨ï¼šè»½é‡å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),    # æ¬ æå€¤è£œå®Œ\n",
    "        ('scaler', StandardScaler()),                     # æ¨™æº–åŒ–\n",
    "        ('feature_selection', SelectKBest(f_classif, k=20))  # ç‰¹å¾´é¸æŠ\n",
    "    ])\n",
    "\n",
    "def train_lecture_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"è¬›ç¾©ç”¨ï¼šè»½é‡ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\"\"\"\n",
    "    print(\"=== ğŸ’» Script Mode: æ©Ÿæ¢°å­¦ç¿’å®Ÿè¡Œé–‹å§‹ ===\")\n",
    "    print(\"â±ï¸ äºˆæƒ³å®Ÿè¡Œæ™‚é–“: 3-6åˆ†\")\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å‰Šæ¸›ï¼ˆè¬›ç¾©ç”¨ï¼‰\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'max_depth': [10, 20]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'learning_rate': [0.1, 0.2]\n",
    "            }\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'model': LogisticRegression(random_state=42, max_iter=500),\n",
    "            'params': {\n",
    "                'C': [0.1, 1.0]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    for name, config in models.items():\n",
    "        print(f\"\\nğŸ“Š {name} ã‚’æœ€é©åŒ–ä¸­...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è»½é‡ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ\n",
    "        grid_search = GridSearchCV(\n",
    "            config['model'],\n",
    "            config['params'],\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # è©•ä¾¡\n",
    "        best_model = grid_search.best_estimator_\n",
    "        train_score = best_model.score(X_train, y_train)\n",
    "        val_score = best_model.score(X_val, y_val)\n",
    "        \n",
    "        # è»½é‡ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=3)\n",
    "        \n",
    "        best_models[name] = best_model\n",
    "        results[name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'train_score': train_score,\n",
    "            'val_score': val_score,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}\")\n",
    "        print(f\"  ğŸ“ˆ æ¤œè¨¼ç²¾åº¦: {val_score:.4f}\")\n",
    "        print(f\"  â±ï¸ è¨“ç·´æ™‚é–“: {training_time:.1f}ç§’\")\n",
    "    \n",
    "    # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['val_score'])\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\nğŸ† æœ€é©ãƒ¢ãƒ‡ãƒ«: {best_model_name}\")\n",
    "    print(f\"ğŸ¯ æ¤œè¨¼ç²¾åº¦: {results[best_model_name]['val_score']:.4f}\")\n",
    "    \n",
    "    return best_model, best_model_name, results\n",
    "\n",
    "# Script Modeå®Ÿè¡Œ\n",
    "print(\"ğŸš€ Script Modeæ©Ÿæ¢°å­¦ç¿’é–‹å§‹...\")\n",
    "script_start_time = time.time()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "X_train = train_data.drop('target', axis=1)\n",
    "y_train = train_data['target']\n",
    "X_val = val_data.drop('target', axis=1)\n",
    "y_val = val_data['target']\n",
    "X_test = test_data.drop('target', axis=1)\n",
    "y_test = test_data['target']\n",
    "\n",
    "print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ç¢ºèª:\")\n",
    "print(f\"  ğŸ“ˆ è¨“ç·´: {X_train.shape}\")\n",
    "print(f\"  ğŸ“‰ æ¤œè¨¼: {X_val.shape}\")\n",
    "print(f\"  ğŸ¯ ãƒ†ã‚¹ãƒˆ: {X_test.shape}\")\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "print(f\"\\nğŸ”§ å‰å‡¦ç†å®Ÿè¡Œä¸­...\")\n",
    "preprocessing_start = time.time()\n",
    "preprocessor = create_lecture_preprocessing_pipeline()\n",
    "X_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "preprocessing_time = time.time() - preprocessing_start\n",
    "\n",
    "print(f\"âœ… å‰å‡¦ç†å®Œäº†: {preprocessing_time:.1f}ç§’\")\n",
    "print(f\"ğŸ“‰ ç‰¹å¾´é‡æ•°: {X_train.shape[1]} â†’ {X_train_processed.shape[1]}\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»æ¯”è¼ƒ\n",
    "script_model, script_best_name, script_results = train_lecture_models(\n",
    "    X_train_processed, y_train, X_val_processed, y_val\n",
    ")\n",
    "\n",
    "script_total_time = time.time() - script_start_time\n",
    "print(f\"\\nğŸ‰ Script Modeå®Œäº†: {script_total_time:.1f}ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Script Modeçµæœã®è©•ä¾¡ã¨å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script Modeãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡\n",
    "print(\"=== ğŸ“Š Script Mode: çµæœè©•ä¾¡ ===\")\n",
    "\n",
    "script_predictions = script_model.predict(X_test_processed)\n",
    "script_accuracy = accuracy_score(y_test, script_predictions)\n",
    "\n",
    "print(f\"ğŸ¯ ãƒ†ã‚¹ãƒˆç²¾åº¦: {script_accuracy:.4f}\")\n",
    "print(f\"\\nğŸ“‹ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:\")\n",
    "print(classification_report(y_test, script_predictions))\n",
    "\n",
    "# çµæœã®å¯è¦–åŒ–\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# æ··åŒè¡Œåˆ—\n",
    "plt.subplot(1, 3, 1)\n",
    "cm = confusion_matrix(y_test, script_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'æ··åŒè¡Œåˆ— - {script_best_name}')\n",
    "plt.xlabel('äºˆæ¸¬')\n",
    "plt.ylabel('å®Ÿéš›')\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆç²¾åº¦ï¼‰\n",
    "plt.subplot(1, 3, 2)\n",
    "model_names = list(script_results.keys())\n",
    "val_scores = [script_results[name]['val_score'] for name in script_results]\n",
    "colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "bars = plt.bar(model_names, val_scores, color=colors)\n",
    "plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥æ¤œè¨¼ç²¾åº¦')\n",
    "plt.ylabel('ç²¾åº¦')\n",
    "plt.ylim(0, 1)\n",
    "# å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "for bar, score in zip(bars, val_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# è¨“ç·´æ™‚é–“æ¯”è¼ƒ\n",
    "plt.subplot(1, 3, 3)\n",
    "training_times = [script_results[name]['training_time'] for name in script_results]\n",
    "bars = plt.bar(model_names, training_times, color=colors)\n",
    "plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥è¨“ç·´æ™‚é–“')\n",
    "plt.ylabel('æ™‚é–“ï¼ˆç§’ï¼‰')\n",
    "# å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "for bar, time_val in zip(bars, training_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Script Modeçµæœã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"  ğŸ† æœ€é©ãƒ¢ãƒ‡ãƒ«: {script_best_name}\")\n",
    "print(f\"  ğŸ¯ ãƒ†ã‚¹ãƒˆç²¾åº¦: {script_accuracy:.4f}\")\n",
    "print(f\"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {script_total_time:.1f}ç§’\")\n",
    "print(f\"  ğŸ”§ å‰å‡¦ç†æ™‚é–“: {preprocessing_time:.1f}ç§’\")\n",
    "print(f\"  ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨“ç·´æ™‚é–“: {sum(training_times):.1f}ç§’\")\n",
    "\n",
    "# Script Modeæƒ…å ±ã‚’ä¿å­˜ï¼ˆæ¯”è¼ƒç”¨ï¼‰\n",
    "script_mode_info = {\n",
    "    'best_model': script_best_name,\n",
    "    'test_accuracy': script_accuracy,\n",
    "    'total_time': script_total_time,\n",
    "    'preprocessing_time': preprocessing_time,\n",
    "    'training_times': training_times,\n",
    "    'model_results': script_results\n",
    "}\n",
    "\n",
    "with open('script_mode_info.json', 'w') as f:\n",
    "    json.dump(script_mode_info, f, indent=2, default=str)\n",
    "    \n",
    "print(f\"\\nğŸ’¾ Script Modeæƒ…å ±ã‚’ä¿å­˜: script_mode_info.json\")\n",
    "print(f\"ğŸš€ æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: 03_training_jobs.ipynb\")\n",
    "print(f\"   åŒã˜S3ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦Training Jobsã‚’å®Ÿè¡Œã—ã¾ã™ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}